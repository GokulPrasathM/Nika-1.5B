# Nika-1.5B

Model link : https://huggingface.co/BeastGokul/Nika-1.5B

Nika 1.5B is a fine-tuned version of the DeepSeek-R1-Distill-Qwen-1.5B model, purpose-built for robust reasoning and mathematical problem-solving. The project leverages the s1K-1.1 dataset, a curated collection of 1,000 reasoning questions with detailed chain-of-thought explanations, to enhance the model’s thought-process clarity and logic generation.

Training is conducted via supervised fine-tuning (SFT) using clear reasoning traces—enabling Nika 1.5B to internalize complex reasoning patterns from the high-quality s1K-1.1 examples. The end result is a lightweight (1.5B parameter) model capable of producing coherent, logically structured responses with strong performance on math and logic benchmarks.

